# Opportunity Scoring System - Technical Documentation

## Table of Contents
1. [What is an Arbitrage Opportunity?](#what-is-an-arbitrage-opportunity)
2. [How Opportunities are Detected](#how-opportunities-are-detected)
3. [Opportunity Scoring System](#opportunity-scoring-system)
4. [Machine Learning vs LLM - Understanding the Difference](#machine-learning-vs-llm---understanding-the-difference)
5. [How Our ML Model Was Trained](#how-our-ml-model-was-trained)
6. [Feature Engineering](#feature-engineering)
7. [Model Architecture](#model-architecture)
8. [Training Process](#training-process)
9. [Prediction Pipeline](#prediction-pipeline)

---

## What is an Arbitrage Opportunity?

An **arbitrage opportunity** exists when the same cryptocurrency token trades at different prices across different blockchain networks (chains). By buying on the cheaper chain and selling on the more expensive chain, traders can profit from this price difference.

### Example
- **ETH on Ethereum**: $4,092.77
- **ETH on Polygon**: $4,085.30
- **Price Difference**: -$7.47 (negative, not profitable)

But if:
- **BNB on BSC**: $1,136.73
- **BNB on Ethereum**: $1,582.86
- **Price Difference**: +$446.13 (positive, potentially profitable!)

However, you must account for **gas costs** (transaction fees) to execute the trade.

---

## How Opportunities are Detected

### Data Pipeline (server/jobs/dataPipeline.ts)

Our system continuously monitors crypto prices through a multi-step process:

1. **CEX Price Fetching** (Centralized Exchanges)
   - Fetches current token prices from CoinGecko API
   - Updates prices for all supported tokens (ETH, BNB, XRP, SOL, MATIC)
   - Stores in MongoDB `tokens` collection

2. **DEX Price Fetching** (Decentralized Exchanges)
   - Queries DexScreener API for on-chain prices
   - Gets actual trading prices from Uniswap, PancakeSwap, QuickSwap
   - Includes liquidity information
   - Updates `dexPrice` field for each token/chain pair

3. **Gas Price Monitoring**
   - Fetches current gas prices for each network
   - Ethereum: via Blocknative API
   - Polygon: via Polygon Gas Station
   - BSC: via BSCScan API

### Opportunity Scanner (server/jobs/opportunityScanner.ts)

Every few minutes, the scanner:

1. **Builds Arbitrage Context**
   - Loads all token prices from database
   - Loads gas prices for all chains
   - Loads native token prices (ETH, BNB, MATIC) for gas cost calculation

2. **Evaluates All Possible Routes**
   - For each token (e.g., BNB)
   - For each pair of chains (e.g., BSC â†’ Ethereum)
   - Calculates if arbitrage is profitable

3. **Calculates Profitability**
   ```typescript
   grossProfit = priceDifference * tradeAmount
   gasCost = (outboundGas + inboundGas) * gasPrice * nativeTokenPrice
   netProfit = grossProfit - gasCost
   profitable = netProfit > 0
   ```

4. **Requests ML Scoring**
   - Sends opportunity details to ML service
   - Gets back a **score** (0.0 to 1.0) indicating quality

5. **Stores in Database**
   - Saves profitable opportunities to MongoDB
   - Marks unprofitable ones as "expired"

---

## Opportunity Scoring System

The **opportunity score** is a number between **0.0** and **1.0** that represents how good an arbitrage opportunity is.

### What the Score Means

- **0.0 - 0.3**: Low quality (barely profitable, high risk)
- **0.3 - 0.6**: Medium quality (decent profit, moderate risk)
- **0.6 - 0.8**: High quality (good profit, lower risk)
- **0.8 - 1.0**: Excellent quality (exceptional profit potential)

### Factors Affecting Score

The ML model considers multiple factors:

1. **Net Profit** (45% weight)
   - Absolute dollar amount after gas costs
   - Higher profit = higher score

2. **ROI - Return on Investment** (25% weight)
   - Profit as percentage of trade volume
   - ROI = (netProfit / tradeVolume) Ã— 100

3. **Gas Efficiency** (20% weight)
   - Ratio of profit to gas costs
   - Higher ratio = better opportunity

4. **Price Momentum** (7% weight)
   - Percentage price difference between chains
   - Larger spreads may indicate stronger trends

5. **Token Recognition** (3% weight)
   - Known tokens vs unknown tokens
   - Supported chains vs unsupported chains

---

## Machine Learning vs LLM - Understanding the Difference

### What We Built: Machine Learning (ML)

**Machine Learning** is a system that learns patterns from data to make predictions. Our system:

- Analyzes historical arbitrage opportunities
- Identifies which factors make opportunities profitable
- Predicts success probability for new opportunities
- Uses **supervised learning** (learns from labeled examples)

### What is an LLM?

**LLM (Large Language Model)** like ChatGPT or GPT-4:

- Understands and generates human language
- Trained on massive text datasets
- Good for conversation, writing, code generation
- **NOT** what we built for opportunity scoring

### Why ML Instead of LLM?

| Aspect | Our ML Model | LLM (like GPT) |
|--------|-------------|----------------|
| **Purpose** | Predict profitability scores | Generate text |
| **Input** | Numerical features (price, gas, ROI) | Text prompts |
| **Output** | Score between 0-1 | Natural language |
| **Training Data** | Real arbitrage opportunities | Books, websites, code |
| **Speed** | Milliseconds | Seconds |
| **Cost** | Free (local) | Expensive (API calls) |
| **Accuracy** | High for numbers | High for language |

**Bottom line**: We use **Machine Learning** because we're predicting numerical outcomes (scores) from numerical data (prices, gas costs), not generating text.

---

## How Our ML Model Was Trained

### Training Data Source

Our model is trained on **REAL opportunity data** from MongoDB, not synthetic/mock data.

**Data Collection Process:**

1. **Run Data Pipeline**
   ```bash
   npx ts-node server/scripts/runPipeline.ts
   ```
   - Fetches live CEX/DEX prices
   - Calculates gas costs
   - Evaluates all arbitrage routes
   - Stores opportunities in database

2. **Opportunity Scanner Generates Training Data**
   - Each opportunity becomes a training example
   - Active opportunities = profitable (label: 1)
   - Expired opportunities = not profitable (label: 0)

3. **Extract Features**
   - Numerical: grossProfit, netProfit, gasCost, ROI, priceDiff, priceDiffPercent, volume
   - Categorical: token symbol, chainFrom, chainTo

### Training Script (llm/train.py)

```python
# Connect to MongoDB
client = MongoClient("mongodb://localhost:27017/MERN-STACK")
db = client.get_database()

# Load opportunities
opportunities = db.opportunities.find({})

# Convert to training data
for opp in opportunities:
    record = {
        "grossProfit": opp["estimatedProfit"],
        "netProfit": opp["netProfit"],
        "gasCost": opp["gasCost"],
        "roi": opp["roi"],
        "profitable": 1 if opp["status"] == "active" else 0
    }
    
# Train Random Forest model
model = RandomForestClassifier(n_estimators=150, max_depth=10)
model.fit(X_train, y_train)
```

---

## Feature Engineering

### Raw Features

Directly from opportunity data:

```typescript
{
  grossProfit: 383.55,      // Price difference Ã— trade amount
  netProfit: 383.86,        // Gross profit - gas costs
  gasCost: 12.34,           // Total gas fees (both chains)
  priceDiff: 446.13,        // Absolute price difference
  priceDiffPercent: 39.2,   // Percentage price difference
  roi: 38.39,               // Return on investment %
  volume: 1000              // Trade size in USD
}
```

### One-Hot Encoded Features

Categorical variables converted to binary:

```python
# Token symbols
symbol_ETH = 1 if token == "ETH" else 0
symbol_BNB = 1 if token == "BNB" else 0
symbol_MATIC = 1 if token == "MATIC" else 0
# ... etc for all tokens

# Chains
chainFrom_ethereum = 1 if chainFrom == "ethereum" else 0
chainFrom_polygon = 1 if chainFrom == "polygon" else 0
chainFrom_bsc = 1 if chainFrom == "bsc" else 0

chainTo_ethereum = 1 if chainTo == "ethereum" else 0
chainTo_polygon = 1 if chainTo == "polygon" else 0
chainTo_bsc = 1 if chainTo == "bsc" else 0
```

### Final Feature Vector

Total: **16 features** (7 numerical + 9 categorical binary)

Example:
```
[383.55, 383.86, 12.34, 446.13, 39.2, 38.39, 1000, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]
```

---

## Model Architecture

### Algorithm: Random Forest Classifier

**Random Forest** is an ensemble of decision trees that vote on predictions.

#### Why Random Forest?

âœ… **Handles non-linear relationships** (price interactions are complex)  
âœ… **Resistant to overfitting** (averages multiple trees)  
âœ… **Works well with small datasets** (we have limited training data)  
âœ… **Provides feature importance** (shows what matters most)  
âœ… **Fast predictions** (milliseconds)  
âœ… **No need for feature scaling** (unlike neural networks)

#### Hyperparameters

```python
RandomForestClassifier(
    n_estimators=150,        # 150 decision trees
    max_depth=10,            # Max 10 levels deep
    min_samples_split=5,     # Need 5+ samples to split
    min_samples_leaf=2,      # Need 2+ samples per leaf
    random_state=42,         # Reproducible results
    class_weight="balanced"  # Handle imbalanced classes
)
```

### Model Output

- **Prediction**: Binary (0 = not profitable, 1 = profitable)
- **Probability**: Float between 0.0 and 1.0 (our **score**)

```python
prediction = model.predict(X)        # [1]
probability = model.predict_proba(X) # [[0.15, 0.85]]
score = probability[0][1]             # 0.85 (85% confident it's profitable)
```

---

## Training Process

### Step-by-Step Workflow

1. **Data Collection**
   ```bash
   npx ts-node server/scripts/runPipeline.ts
   ```
   - Fetches prices, evaluates opportunities
   - Stores in MongoDB

2. **Model Training**
   ```bash
   cd llm
   python train.py
   ```
   
   Console output:
   ```
   ðŸ“¡ Connecting to MongoDB: mongodb://localhost:27017/MERN-STACK
   ðŸ“Š Found 12 opportunities in database
   âœ… Loaded 12 training samples
      Profitable: 5
      Not profitable: 7
   
   ðŸ”§ Engineering features...
      Features: 16
      Samples: 12
   
   ðŸ¤– Training Random Forest model...
   âœ… Training accuracy: 0.778
   âœ… Test accuracy: 0.667
   
   ðŸ“Š Top 10 Most Important Features:
              feature  importance
              gasCost    0.146032
     chainTo_ethereum    0.123575
   chainFrom_ethereum    0.122082
                  roi    0.119085
     priceDiffPercent    0.097602
            netProfit    0.090912
   
   ðŸ’¾ Saved model to models/arbitrage_model.pkl
   ```

3. **Model Deployment**
   ```bash
   cd llm
   uvicorn service:app --reload
   ```
   - Starts FastAPI server on port 8000
   - Loads trained model
   - Exposes `/predict` endpoint

4. **Re-score Opportunities**
   ```bash
   npx ts-node server/scripts/rescanOpportunities.ts
   ```
   - Recalculates scores for all opportunities using trained model

---

## Prediction Pipeline

### Real-Time Scoring Flow

```
1. New Opportunity Detected
   â””â”€> ArbitrageService evaluates profitability
       â””â”€> Calculates grossProfit, netProfit, gasCost, ROI

2. Request ML Score
   â””â”€> MLService.getPrediction({
         token: "BNB",
         chain: "bsc",
         grossProfit: 383.55,
         netProfit: 383.86,
         gasCost: 12.34,
         roi: 38.39,
         priceDiffPercent: 39.2
       })

3. ML Service (Python FastAPI)
   â””â”€> predict.py receives request
       â””â”€> Loads trained model
           â””â”€> Prepares feature vector
               â””â”€> model.predict_proba(X)
                   â””â”€> Returns score: 0.85

4. Store with Score
   â””â”€> Opportunity saved to MongoDB with score=0.85
       â””â”€> Frontend displays high-quality opportunity
```

### API Contract

**Request:**
```json
POST http://localhost:8000/predict
{
  "token": "BNB",
  "chain": "bsc",
  "price": 383.55,
  "gas": 12.34,
  "grossProfit": 383.55,
  "netProfit": 383.86,
  "roi": 38.39,
  "tradeVolume": 1000,
  "priceDiffPercent": 39.2,
  "pricePerToken": 446.13
}
```

**Response:**
```json
{
  "profitable": true,
  "roi": 38.39,
  "score": 0.85,
  "metadata": {
    "tokenRecognized": true,
    "chainRecognized": true,
    "modelUsed": "RandomForest"
  }
}
```

---

## Summary

### What We Built

âœ… **Machine Learning Model** (not an LLM)  
âœ… **Trained on Real Data** (no mocks, no fallbacks)  
âœ… **Random Forest Classifier** (150 decision trees)  
âœ… **16 Features** (7 numerical, 9 categorical)  
âœ… **Binary Classification** (profitable vs not profitable)  
âœ… **Probability Output** (0.0-1.0 score)  

### Key Differences: ML vs LLM

| Feature | Our ML Model | LLM |
|---------|--------------|-----|
| **What it does** | Predicts opportunity quality scores | Generates human-like text |
| **Input type** | Numbers (prices, gas, ROI) | Text (prompts, questions) |
| **Output type** | Probability score (0.0-1.0) | Text (answers, code, stories) |
| **Training data** | Arbitrage opportunities from MongoDB | Internet text, books, code |
| **Model type** | Random Forest (ensemble trees) | Transformer neural network |
| **Size** | ~50KB (model file) | 100GB+ (GPT-4) |
| **Speed** | 1-5ms per prediction | 1-10s per response |
| **Cost** | Free (runs locally) | $0.03+ per 1K tokens |

### Why ML is Perfect for This

1. **Numerical Predictions** - We predict scores, not text
2. **Fast & Efficient** - Sub-second responses for live trading
3. **Interpretable** - We know which features matter most
4. **Cost-Effective** - No API fees, runs on our server
5. **Data-Driven** - Learns from actual market behavior

---

## Next Steps

### Improving the Model

1. **Collect More Data**
   - Run pipeline for longer periods
   - Accumulate hundreds of opportunities
   - Better training distribution

2. **Feature Enhancement**
   - Add liquidity metrics
   - Include time-of-day patterns
   - Track historical volatility

3. **Model Tuning**
   - Cross-validation for hyperparameters
   - Test different algorithms (XGBoost, Neural Networks)
   - Ensemble multiple models

4. **Production Monitoring**
   - Track prediction accuracy
   - Log false positives/negatives
   - Retrain periodically with new data

---

## File Structure

```
llm/
â”œâ”€â”€ train.py                  # Training script (connects to MongoDB)
â”œâ”€â”€ predict.py                # Prediction logic (loads trained model)
â”œâ”€â”€ service.py                # FastAPI server (exposes /predict endpoint)
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ tokens_config.py          # Supported tokens/chains
â””â”€â”€ models/
    â”œâ”€â”€ arbitrage_model.pkl           # Trained Random Forest model
    â””â”€â”€ arbitrage_model_features.txt  # Feature names (must match training)
```

---

**Last Updated**: October 26, 2025  
**Model Version**: v1.0 (Random Forest)  
**Training Samples**: 12 real opportunities  
**Test Accuracy**: 66.7%
